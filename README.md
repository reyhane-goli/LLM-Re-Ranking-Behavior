# LLM-Re-Ranking-Behavior

Large language models (LLMs) are effective zero-shot re-rankers, producing a relevance-ordered list by prompting the model to re- order an initial set of retrieved documents; and doing so without any task-specific training. However, understanding their behavior is not straightforward. Re-ranking involves set-wise comparisons across multiple candidates, but with practical systems processing candidate lists in smaller windows to satisfy context-length con straints. Further variability arises from run-to-run variation, and from changes in the deployed models over time. Together, these factors motivate systematic exploration of how candidate-set construction and presentation shape LLM re-ranking behavior.

We have reimplemented the BM25 retrieval stage, the supervised re-rankers, and the sliding-window LLM re-ranking stage. We confirm that supervised and LLM re-rankers improve over BM25, and LLM-based approaches often achieve the strongest effectiveness. Then, moving beyond reproduction, we study how candidate set properties shape LLM re-ranking behavior via four research questions: (RQ1) whether effectiveness varies with input length and (RQ2) whether the choice of first-stage retriever affects effectiveness; and then, in experiments using only the biomedical data, (RQ3) whether stability and effectiveness vary with input size, and (RQ4) how positioning of relevant documents in the input list affects re-ranking. We find that moderate candidate depths are the most reliable; that the first stage exerts a notable effect; and that LLM re-rankers are sensitive to list length and presentation order.
