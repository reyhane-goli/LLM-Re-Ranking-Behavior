# """
# UPR (Unsupervised Passage Re-ranking) reproduction, FLAN-T5-XL
# Reference: Sachan et al., 2022
# """

# import os, torch
# from tqdm import tqdm
# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# PROMPT_TEMPLATE = "Generate a question relevant to this passage:\n\n{doc}\n\nQuestion:"

# class UPRReRanker:
#     def __init__(self, model_name="google/flan-t5-xl", device=None):
#         print(f"[INFO] Loading UPR model: {model_name}")
#         self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)
#         self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32)
#         self.model.to(self.device)
#         self.model.eval()

#     def generate_queries(self, docs, max_length=64):
#         prompts = [PROMPT_TEMPLATE.format(doc=d) for d in docs]
#         inputs = self.tokenizer(prompts, padding=True, truncation=True, return_tensors="pt").to(self.device)
#         with torch.no_grad():
#             outputs = self.model.generate(**inputs, max_length=max_length)
#         return [self.tokenizer.decode(o, skip_special_tokens=True) for o in outputs]
    
#     def rerank(self, query, candidate_docs, searcher, k=1):
#         """
#         Re-rank BM25 docs using pseudo-queries generated by FLAN-T5-XL.
#         This version uses Pyserini's LuceneSearcher to score pseudo-queries against docs.
#         """
#         docs_texts, docs_ids = [], []
#         for d in candidate_docs:
#             if isinstance(d, dict):
#                 docs_ids.append(d.get('docid', ''))
#                 docs_texts.append(d.get('text', ''))
#             elif isinstance(d, tuple) and len(d) >= 2:
#                 docs_ids.append(d[0])
#                 docs_texts.append(d[1])
#             else:
#                 raise ValueError(f"Unexpected document format: {type(d)}")

#         # --- Step 1: Generate pseudo-queries ---
#         pseudo_queries = self.generate_queries(docs_texts)

#         # --- Step 2: Score each document by BM25 similarity to its pseudo-query ---
#         scores = []
#         for doc_id, pq in zip(docs_ids, pseudo_queries):
#             # Perform BM25 scoring: query against that single doc
#             hits = searcher.search(pq, k=1)
#             # If the original doc appears in results, use its score; else assign 0
#             score = next((h.score for h in hits if h.docid == doc_id), 0.0)
#             scores.append((doc_id, score))

#         # --- Step 3: Sort by BM25 similarity descending ---
#         return sorted(scores, key=lambda x: x[1], reverse=True)


"""
UPR (Unified Passage Reranker) — Fixed Version
Supports:
  (1) Unsupervised UPR (Flan-T5 style pseudo-queries)
  (2) Supervised UPR / MonoT5 (MS MARCO fine-tuned scoring)
"""

import torch
from torch.nn.functional import log_softmax
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

PROMPT_UNSUP = "Generate a question relevant to this passage:\n\n{doc}\n\nQuestion:"
PROMPT_SUP   = "Query: {query} Document: {doc} Relevant:"

class UPRReRanker:
    def __init__(self, model_name="google/flan-t5-xl", device=None):
        print(f"[INFO] Loading UPR model: {model_name}")
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32
        ).to(self.device)
        self.model.eval()

        self.supervised = any(x in model_name.lower() for x in ["monot5", "upr-msmarco"])
        print(f"[INFO] Mode: {'Supervised MonoT5' if self.supervised else 'Unsupervised Flan-T5'}")

    def generate_queries(self, docs, max_length=64):
        prompts = [PROMPT_UNSUP.format(doc=d) for d in docs]
        inputs = self.tokenizer(prompts, padding=True, truncation=True, return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.model.generate(**inputs, max_length=max_length)
            return [self.tokenizer.decode(o, skip_special_tokens=True) for o in outputs]
        
    def score_monot5(self, query, docs):
        """
        Compute MonoT5-style log-likelihood for 'true' token.
        This matches the official PyGaggle / Castorini implementation.
        """
        scores = []
        batch_size = 8  # adjust if GPU memory is small

        for i in range(0, len(docs), batch_size):
            batch = docs[i:i + batch_size]
            prompts = [f"Query: {query} Document: {d} Relevant:" for d in batch]

            enc = self.tokenizer(
                prompts,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="pt"
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model.generate(
                    **enc,
                    max_new_tokens=1,
                    return_dict_in_generate=True,
                    output_scores=True
                )

            # Get logits for the generated token (first step)
            step_scores = outputs.scores[0]
            log_probs = torch.nn.functional.log_softmax(step_scores, dim=-1)

            true_id = self.tokenizer.encode("true", add_special_tokens=False)[0]
            batch_scores = log_probs[:, true_id]
            scores.extend(batch_scores.detach().cpu().tolist())

        return scores


    def clean_doc_text(text):
        # Remove boilerplate and truncate to 512 tokens for T5
        text = text.replace('\n', ' ').replace('\t', ' ')
        text = text.replace('<', ' ').replace('>', ' ')
        text = ' '.join(text.split())
        return text[:4000]  # ≈ 512 tokens (safe cutoff)


    def rerank(self, query, candidate_docs, searcher=None, k=1):
        docs_texts, docs_ids = [], []
        
        
#         for d in candidate_docs:
#             if isinstance(d, dict):
#                 docs_ids.append(d.get('docid', ''))
# #                 docs_texts.append(d.get('text', ''))
#                 docs_texts.append(clean_doc_text(d.get('text', '')))

                
#                 if len(docs_texts[-1]) < 20:
#                     print("[WARN] very short doc:", docs_texts[-1][:100])
                    
#             elif isinstance(d, tuple) and len(d) >= 2:
#                 docs_ids.append(d[0])
#                 docs_texts.append(d[1])
#             else:
#                 raise ValueError(f"Unexpected document format: {type(d)}")

        import json

        for d in candidate_docs:
            docid = d.get('docid', '') if isinstance(d, dict) else d[0]
            raw = d.get('text', '') if isinstance(d, dict) else d[1]

            # Try to decode if Lucene stored JSON
            try:
                raw_json = json.loads(raw)
                text = raw_json.get('contents', raw)
            except Exception:
                text = raw

            # Clean + truncate for T5
             # Ensure text is a string before cleaning
            if not isinstance(text, str):
                text = str(text) if text is not None else ""

            text = text.replace('\n', ' ').replace('\t', ' ')
            text = ' '.join(text.split())
            text = text[:4000]


            docs_ids.append(docid)
            docs_texts.append(text)


        if self.supervised:
            scores = self.score_monot5(query, docs_texts)
            return sorted(zip(docs_ids, scores), key=lambda x: x[1], reverse=True)

        # Unsupervised fallback
        pseudo_queries = self.generate_queries(docs_texts)
        scores = []
        for doc_id, pq in zip(docs_ids, pseudo_queries):
            hits = searcher.search(pq, k=1)
            score = next((h.score for h in hits if h.docid == doc_id), 0.0)
            scores.append((doc_id, score))
        return sorted(scores, key=lambda x: x[1], reverse=True)
